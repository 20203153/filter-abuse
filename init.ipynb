{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cbc581ff67444d1bd8bc9b2ba25760d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading JSON files:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a8ad237fb314729a0199a031170a6d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "One Hot Encoding:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  INSULT  PROFANE  \\\n",
      "0  국민들은 깨어나야 합니다. 지금 민주당을 지지하는 사림들 보세요~ 종북주의자들이 판...   0.625    0.000   \n",
      "1                           저 어른들...지금 정부 꼬라지 보는것같구만   0.750    0.000   \n",
      "2  대깨문 출동 이번 코로나가 확산되는건 민노총 때문이라고 몰고가라~~~ 눼눼~ 출동출...   0.750    0.000   \n",
      "3                                 아프간과 비교를...ㅎ 치매드셨나   0.750    0.000   \n",
      "4  작년 국민집회때 #@이름#목사 감옥보내더니 ㅋㅋㅋ8000명 민노총은 집회허락하는 이...   0.625    0.000   \n",
      "5  태권도는 졸라 깨지고~창피하다!!!!!!태권도 종주국 이라고 어디가서 떠들지 마라!...   0.750    0.625   \n",
      "6  더불당은 동네 똥개 크게 소리 짓는 암숫개들 다 모아 놓았네 오늘 초복인데 도살장 ...   0.750    0.000   \n",
      "7   미국이 지원해서 가장 성공한 케이스가 대한민국.그리고 잘살게되니까 반미하는 토종빨갱이들   0.750    0.000   \n",
      "8                               세금 그렇게 먹으면 죽을만 하지 ㅇㅇ   0.625    0.000   \n",
      "9  또다시 더불어당 #@이름# #@이름#이 입을 맞추며 분위기를 잡고 있다. 싹 쓸어버...   0.625    0.000   \n",
      "\n",
      "   OBSCENE  THREAT  HATE_SPEECH  AGEISM  RACISM  DISABILITY  RELIGION  \\\n",
      "0      0.0     0.0          0.0     0.0     0.0       0.000       0.0   \n",
      "1      0.0     0.0          0.0     0.0     0.0       0.000       0.0   \n",
      "2      0.0     0.0          0.0     0.0     0.0       0.000       0.0   \n",
      "3      0.0     0.0          0.0     0.0     0.0       0.625       0.0   \n",
      "4      0.0     0.0          0.0     0.0     0.0       0.000       0.0   \n",
      "5      0.0     0.0          0.0     0.0     0.0       0.000       0.0   \n",
      "6      0.0     0.0          0.0     0.0     0.0       0.000       0.0   \n",
      "7      0.0     0.0          0.0     0.0     0.0       0.000       0.0   \n",
      "8      0.0     0.0          0.0     0.0     0.0       0.000       0.0   \n",
      "9      0.0     0.0          0.0     0.0     0.0       0.000       0.0   \n",
      "\n",
      "   POLITICAL  PROFESSION  \n",
      "0      0.625         0.0  \n",
      "1      0.625         0.0  \n",
      "2      0.875         0.0  \n",
      "3      0.000         0.0  \n",
      "4      0.625         0.0  \n",
      "5      0.000         0.0  \n",
      "6      0.875         0.0  \n",
      "7      0.750         0.0  \n",
      "8      0.000         0.0  \n",
      "9      0.625         0.0  \n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "\n",
    "def load_json_files(path_pattern):\n",
    "    files = glob.glob(path_pattern)\n",
    "    data = []\n",
    "    for file in tqdm(files, \"Loading JSON files\"):\n",
    "        with open(file, 'r', encoding='utf-8-sig') as f:\n",
    "            json_data = json.load(f)\n",
    "            data.append(json_data)\n",
    "    return data\n",
    "\n",
    "def remove_highlights(text: str) -> str:\n",
    "    \"\"\"하이라이트를 제거하고 연속된 같은 문자를 하나로 축소하는 함수\"\"\"\n",
    "    # 하이라이트 제거\n",
    "    clean_text = re.sub(r\"''(.*?)'''\", \"\\\\1\", text.strip())\n",
    "    \n",
    "    # 연속된 같은 문자를 하나로 축소\n",
    "    clean_text = re.sub(r'(.)\\1+', r'\\1', clean_text)\n",
    "\n",
    "    return clean_text\n",
    "\n",
    "def create_one_hot_encoding(data):\n",
    "    \"\"\"One-hot encoding DataFrame을 생성하는 함수\"\"\"\n",
    "    # 기본 데이터프레임 생성\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # 라벨 목록 (영어 대문자로 수정)\n",
    "    label_categories = [\n",
    "        \"INSULT\",  # 모욕\n",
    "        \"PROFANE\",  # 욕설\n",
    "        \"OBSCENE\",  # 외설\n",
    "        \"THREAT\",  # 폭력위협/범죄조장\n",
    "        \"HATE_SPEECH\",  # 성혐오\n",
    "        \"AGEISM\",  # 연령\n",
    "        \"RACISM\",  # 인종/지역\n",
    "        \"DISABILITY\",  # 장애\n",
    "        \"RELIGION\",  # 종교\n",
    "        \"POLITICAL\",  # 정치성향\n",
    "        \"PROFESSION\"  # 직업\n",
    "    ]\n",
    "\n",
    "    labels = [\n",
    "        \"모욕\",\n",
    "        \"욕설\",\n",
    "        \"외설\",\n",
    "        \"폭력위협/범죄조장\",\n",
    "        \"성혐오\",\n",
    "        \"연령\",\n",
    "        \"인종/지역\",\n",
    "        \"장애\",\n",
    "        \"종교\",\n",
    "        \"정치성향\",\n",
    "        \"직업\"\n",
    "    ]\n",
    "\n",
    "    # One-hot encoding DataFrame 초기화\n",
    "    one_hot_labels = pd.DataFrame(0, index=df.index, columns=label_categories, dtype='float32')\n",
    "\n",
    "    # 'text' 하이라이트 처리 및 Labels 설정\n",
    "    for index, row in tqdm(df.iterrows(), \"One Hot Encoding\", total=len(df)):\n",
    "        # \"문장\" 열에서 하이라이트 제거\n",
    "        df.loc[index, '문장'] = remove_highlights(row['문장'])\n",
    "        \n",
    "        for idx, category in enumerate(labels):\n",
    "            if category in row and row[category] >= 1:\n",
    "                one_hot_labels.at[index, label_categories[idx]] = row[category] * 0.125 + 0.5\n",
    "    \n",
    "    # 최종 DataFrame 생성\n",
    "    saved = pd.concat([df[['문장']], one_hot_labels], axis=1)  # 'text' 열 포함\n",
    "    saved.rename(columns={'문장': 'text'}, inplace=True)\n",
    "    \n",
    "    return saved\n",
    "\n",
    "# JSON 파일 로드\n",
    "json_files_path = 'data/selectstar/C*.json'\n",
    "data = load_json_files(json_files_path)\n",
    "\n",
    "# 원본 데이터로부터 One-hot encoding DataFrame 생성\n",
    "one_hot_df = create_one_hot_encoding(data)\n",
    "\n",
    "# 결과 출력 및 CSV 파일로 저장(optional)\n",
    "print(one_hot_df.head(n=10))\n",
    "one_hot_df.to_csv('selectstar-encoded.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, f1_score, hamming_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import DatasetDict, Dataset\n",
    "import os\n",
    "\n",
    "# 레이블 설정 (영어로 의미 표현)\n",
    "label_categories = [\n",
    "    \"INSULT\",  # 모욕\n",
    "    \"PROFANE\",  # 욕설\n",
    "    \"OBSCENE\",  # 외설\n",
    "    \"THREAT\",  # 폭력위협/범죄조장\n",
    "    \"HATE_SPEECH\",  # 성혐오\n",
    "    \"AGEISM\",  # 연령\n",
    "    \"RACISM\",  # 인종/지역\n",
    "    \"DISABILITY\",  # 장애\n",
    "    \"RELIGION\",  # 종교\n",
    "    \"POLITICAL\",  # 정치성향\n",
    "    \"PROFESSION\"  # 직업\n",
    "]\n",
    "\n",
    "# label2id 및 id2label 정의\n",
    "label2id = {label: i for i, label in enumerate(label_categories)}\n",
    "id2label = {i: label for i, label in enumerate(label_categories)}\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    # 데이터 로드\n",
    "    raw_dataset = pd.read_csv('./selectstar-encoded.csv')\n",
    "\n",
    "    # 레이블을 One-hot Encoding 형식으로 변환하기\n",
    "    def create_labels(row):\n",
    "        return [1 if row[category] > 0 else 0 for category in label_categories]\n",
    "\n",
    "    # 레이블 데이터 생성\n",
    "    raw_dataset['labels'] = raw_dataset.apply(create_labels, axis=1)\n",
    "\n",
    "    # 데이터 분할\n",
    "    X_train, X_eval, Y_train, Y_eval = train_test_split(\n",
    "        raw_dataset['문장'].tolist(), \n",
    "        raw_dataset['labels'].tolist(), \n",
    "        test_size=0.2, \n",
    "        random_state=42,  # 재현성을 위해 시드 고정 \n",
    "        stratify=raw_dataset['labels']  # 레이블에 따라 stratified 분할\n",
    "    )\n",
    "\n",
    "    return X_train, X_eval, Y_train, Y_eval\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # CUDA, MPS, CPU 체크\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # 모델과 토크나이저 로드\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"beomi/KcELECTRA-base\", trust_remote_code=True)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"beomi/KcELECTRA-base\",\n",
    "        num_labels=len(label_categories),  # 레이블 수\n",
    "        id2label=id2label,  # id2label 설정\n",
    "        label2id=label2id,  # label2id 설정\n",
    "        problem_type=\"multi_label_classification\"\n",
    "    ).to(device)\n",
    "    \n",
    "    # 데이터셋 로드 및 변환\n",
    "    X_train, X_eval, Y_train, Y_eval = load_and_preprocess_data()\n",
    "\n",
    "    # Tokenization 및 인덱싱\n",
    "    train_encodings = tokenizer(X_train, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    eval_encodings = tokenizer(X_eval, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    # PyTorch Dataset 정의\n",
    "    class MultiLabelDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, encodings, labels):\n",
    "            self.encodings = encodings\n",
    "            self.labels = labels\n",
    "            \n",
    "        def __getitem__(self, idx):\n",
    "            item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)  # 멀티 레이블인 경우 float형으로 변경\n",
    "            return item\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.labels)\n",
    "\n",
    "    # 데이터셋 생성\n",
    "    train_dataset = MultiLabelDataset(train_encodings, Y_train)\n",
    "    eval_dataset = MultiLabelDataset(eval_encodings, Y_eval)\n",
    "\n",
    "    # Trainer 설정\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        num_train_epochs=5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=100,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"steps\",\n",
    "        save_total_limit=3,\n",
    "        save_steps=1000,\n",
    "        learning_rate=5e-5,\n",
    "        fp16=torch.cuda.is_available(),  # FP16 활성화 GPU의 경우\n",
    "    )\n",
    "\n",
    "    # Trainer 정의\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "    )\n",
    "\n",
    "    # 모델 학습\n",
    "    trainer.train()\n",
    "\n",
    "    # 모델 평가\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(eval_results)\n",
    "\n",
    "    # 테스트 데이터셋 평가\n",
    "    predictions = trainer.predict(eval_dataset)\n",
    "    pred_labels = (torch.sigmoid(torch.tensor(predictions.predictions)) >= 0.5).cpu().numpy()\n",
    "    \n",
    "    test_labels_np = np.vstack(eval_dataset.labels)\n",
    "\n",
    "    # Classification report 출력\n",
    "    print(classification_report(test_labels_np, pred_labels, target_names=label_categories))\n",
    "\n",
    "    # 모델 및 토크나이저 저장\n",
    "    model_save_path = \"./saved_model\"\n",
    "    os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "    trainer.save_model(model_save_path)\n",
    "    tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "    print(f\"Model and tokenizer saved to {model_save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "filter-abuse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
