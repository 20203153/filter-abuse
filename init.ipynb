{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2acf559cbab43e19edc741bc3e27183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Files:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  문장\n",
      "0  국민들은 깨어나야 합니다. 지금 민주당을 지지하는 사림들 보세요~ 종북주의자들이 판...\n",
      "1                      저 어른들...지금 정부 ''꼬라지''' 보는것같구만\n",
      "2  ''대깨문''' 출동 이번 코로나가 확산되는건 민노총 때문이라고 몰고가라~~~ 눼눼...\n",
      "3                            아프간과 비교를...ㅎ ''치매드셨'''나\n",
      "4  작년 국민집회때 #@이름#목사 감옥보내더니 ㅋㅋㅋ8000명 민노총은 집회허락하는 이...\n",
      "5  태권도는 ''졸라''' ''깨지'''고~''창피하'''다!!!!!!태권도 종주국 이...\n",
      "6  ''더불당'''은 동네 ''똥개''' 크게 ''소리''' ''짓'''는 ''암숫개'...\n",
      "7  미국이 지원해서 가장 성공한 케이스가 대한민국.그리고 잘살게되니까 ''반미하'''는...\n",
      "8                               세금 그렇게 먹으면 죽을만 하지 ㅇㅇ\n",
      "9  또다시 더불어당 #@이름# #@이름#이 입을 맞추며 분위기를 잡고 있다. 싹 쓸어버...\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "\n",
    "def load_json_files(path_pattern):\n",
    "    files = glob.glob(path_pattern)\n",
    "    data = []\n",
    "    for file in tqdm(files, \"Files\"):\n",
    "        with open(file, 'r', encoding='utf-8-sig') as f:\n",
    "            json_data = json.load(f)\n",
    "            data.append(json_data)\n",
    "    return data\n",
    "\n",
    "def create_one_hot_encoding(data):\n",
    "    # 기본 데이터프레임 생성\n",
    "    df = pd.DataFrame(data)\n",
    "    saved = pd.DataFrame()\n",
    "    \n",
    "    # 원문 및 위치 정보 추가\n",
    "    saved['문장'] = df['문장']\n",
    "\n",
    "    return saved\n",
    "\n",
    "# JSON 파일 로드\n",
    "json_files_path = 'data/selectstar/C*.json'\n",
    "data = load_json_files(json_files_path)\n",
    "\n",
    "# 원본 데이터로부터 One-hot encoding DataFrame 생성\n",
    "one_hot_df = create_one_hot_encoding(data)\n",
    "\n",
    "# 결과 출력 및 CSV 파일로 저장(optional)\n",
    "print(one_hot_df.head(n=10))\n",
    "one_hot_df.to_csv('selectstar-encoded.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      문장  \\\n",
      "0      국민들은 깨어나야 합니다. 지금 민주당을 지지하는 사림들 보세요~ 종북주의자들이 판...   \n",
      "1                          저 어른들...지금 정부 ''꼬라지''' 보는것같구만   \n",
      "2      ''대깨문''' 출동 이번 코로나가 확산되는건 민노총 때문이라고 몰고가라~~~ 눼눼...   \n",
      "3                                아프간과 비교를...ㅎ ''치매드셨'''나   \n",
      "4      작년 국민집회때 #@이름#목사 감옥보내더니 ㅋㅋㅋ8000명 민노총은 집회허락하는 이...   \n",
      "...                                                  ...   \n",
      "99995  \\n판결 요약하면 ''기레기'''질은 맞는데 언론의 자유를 위해 봐준다임. 나쁜''...   \n",
      "99996  나라를 위해 이렇게 몸 아끼지않고 헌신하는 애국자가 있는반면 파란지붕아래 ''쓰레기...   \n",
      "99997  정권바뀌면 없어져야할것,,,,아무쓸모없고 25만원 받아야 아무도움안되니 세금이나 그...   \n",
      "99998  그''놈'''의 #@이름#이가 뭐라고 저토록 지긋지긋하게 무죄가 만들어 주고싶어 그...   \n",
      "99999  ''쌍욕달이''' ''깡패''' #@이름#과 ''망언''' ''전문''' ''문빠'...   \n",
      "\n",
      "                                                   욕설 위치  \n",
      "0                                           [(195, 203)]  \n",
      "1                                             [(14, 21)]  \n",
      "2                       [(0, 7), (129, 136), (229, 236)]  \n",
      "3                                             [(13, 21)]  \n",
      "4                                                     []  \n",
      "...                                                  ...  \n",
      "99995            [(9, 16), (43, 48), (57, 62), (68, 73)]  \n",
      "99996                             [(41, 48), (128, 135)]  \n",
      "99997                                         [(56, 62)]  \n",
      "99998  [(1, 6), (47, 55), (70, 77), (138, 143), (144,...  \n",
      "99999   [(0, 8), (10, 16), (25, 31), (33, 39), (41, 47)]  \n",
      "\n",
      "[100000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 예시 DataFrame 생성\n",
    "df = pd.read_csv(\"selectstar-encoded.csv\", index_col=0)\n",
    "\n",
    "def extract_profanity_positions(sentence):\n",
    "    # '(욕설)' 형태로 마킹된 모든 욕설을 찾기 위한 정규 표현식\n",
    "    pattern = r\"''(.*?)'''\"\n",
    "    \n",
    "    # 욕설 위치 저장할 리스트\n",
    "    positions = []\n",
    "    \n",
    "    # 매칭된 모든 욕설에 대해 시작과 끝 인덱스를 계산\n",
    "    for match in re.finditer(pattern, sentence):\n",
    "        start = match.start(0)\n",
    "        end = match.end(0) - 1  # end 인덱스는 포함하지 않으므로 -1\n",
    "        positions.append((start, end))\n",
    "        \n",
    "    return positions\n",
    "\n",
    "# 각 문장에 대해 시작/종료 인덱스 추가\n",
    "df['욕설 위치'] = df['문장'].apply(extract_profanity_positions)\n",
    "# 결과 출력\n",
    "print(df[['문장', '욕설 위치']])\n",
    "df.to_csv('./selectstar-location.csv', encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "model_name = \"monologg/kobert\"  # 또는 \"roberta-base\" 등\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=2)  # num_labels는 레이블 수\n",
    "raw_dataset = pd.read_csv('./selectstar-location.csv')\n",
    "\n",
    "def create_labels(sentence, positions):\n",
    "    tokens = list(sentence)  # 문장을 문자 단위로 토큰화\n",
    "    labels = ['O'] * len(tokens)  # 초기 레이블 설정\n",
    "\n",
    "    if not positions:  # positions가 비어있는 경우\n",
    "        return tokens, labels\n",
    "\n",
    "    for start, end in positions:\n",
    "        labels[start] = 'B-PROFANITY'\n",
    "        for i in range(start + 1, end + 1):\n",
    "            if i < len(labels):  # 인덱스가 범위를 벗어나지 않도록\n",
    "                labels[i] = 'I-PROFANITY'\n",
    "\n",
    "    return tokens, labels\n",
    "\n",
    "# 데이터셋 생성하기\n",
    "dataset = []\n",
    "for index, row in raw_dataset.iterrows():\n",
    "    tokens, labels = create_labels(row['문장'], row['욕설 위치'])\n",
    "    dataset.append((tokens, labels))\n",
    "\n",
    "texts, labels = zip(*dataset)\n",
    "\n",
    "X_train, X_eval, Y_train, Y_eval = train_test_split(texts, labels, test_size=0.2)\n",
    "\n",
    "# Tokenization 및 인덱싱\n",
    "train_encodings = tokenizer(list(X_train), is_split_into_words=True, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "eval_encodings = tokenizer(list(X_eval), is_split_into_words=True, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# 레이블을 인덱스로 변환\n",
    "label_to_index = {'O': 0, 'B-PROFANITY': 1, 'I-PROFANITY': 2}\n",
    "train_labels = [[label_to_index[label] for label in label_list] for label_list in Y_train]\n",
    "eval_labels = [[label_to_index[label] for label in label_list] for label_list in Y_eval]\n",
    "\n",
    "class ProfanityDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "# 데이터셋 생성\n",
    "train_dataset = ProfanityDataset(train_encodings, train_labels)\n",
    "eval_dataset = ProfanityDataset(eval_encodings, eval_labels)\n",
    "\n",
    "# Trainer 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=16,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=5,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "filter-abuse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
