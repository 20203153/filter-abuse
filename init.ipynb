{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d422a1ca4a646929db24f57f32a454c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Files:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3779601e82d046c9862dcb0674b089ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Labels:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   모욕  욕설  외설  폭력위협/범죄조장  성혐오  연령  인종/지역  장애  종교  정치성향  직업  \\\n",
      "0   0   0   0          0    0   0      0   0   0     0   0   \n",
      "1   0   0   0          0    0   0      0   0   0     0   0   \n",
      "2   0   0   0          0    0   0      0   0   0     0   0   \n",
      "3   0   0   0          0    0   0      0   0   0     0   0   \n",
      "4   0   0   0          0    0   0      0   0   0     0   0   \n",
      "5   0   0   0          0    0   0      0   0   0     0   0   \n",
      "6   0   0   0          0    0   0      0   0   0     0   0   \n",
      "7   0   0   0          0    0   0      0   0   0     0   0   \n",
      "8   0   0   0          0    0   0      0   0   0     0   0   \n",
      "9   0   0   0          0    0   0      0   0   0     0   0   \n",
      "\n",
      "                                                  문장  \n",
      "0  국민들은 깨어나야 합니다. 지금 민주당을 지지하는 사림들 보세요~ 종북주의자들이 판...  \n",
      "1                      저 어른들...지금 정부 ''꼬라지''' 보는것같구만  \n",
      "2  ''대깨문''' 출동 이번 코로나가 확산되는건 민노총 때문이라고 몰고가라~~~ 눼눼...  \n",
      "3                            아프간과 비교를...ㅎ ''치매드셨'''나  \n",
      "4  작년 국민집회때 #@이름#목사 감옥보내더니 ㅋㅋㅋ8000명 민노총은 집회허락하는 이...  \n",
      "5  태권도는 ''졸라''' ''깨지'''고~''창피하'''다!!!!!!태권도 종주국 이...  \n",
      "6  ''더불당'''은 동네 ''똥개''' 크게 ''소리''' ''짓'''는 ''암숫개'...  \n",
      "7  미국이 지원해서 가장 성공한 케이스가 대한민국.그리고 잘살게되니까 ''반미하'''는...  \n",
      "8                               세금 그렇게 먹으면 죽을만 하지 ㅇㅇ  \n",
      "9  또다시 더불어당 #@이름# #@이름#이 입을 맞추며 분위기를 잡고 있다. 싹 쓸어버...  \n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "\n",
    "def load_json_files(path_pattern):\n",
    "    files = glob.glob(path_pattern)\n",
    "    data = []\n",
    "    for file in tqdm(files, \"Files\"):\n",
    "        with open(file, 'r', encoding='utf-8-sig') as f:\n",
    "            json_data = json.load(f)\n",
    "            data.append(json_data)\n",
    "    return data\n",
    "\n",
    "def create_one_hot_encoding(data):\n",
    "    # 기본 데이터프레임 생성\n",
    "    df = pd.DataFrame(data)\n",
    "    saved = pd.DataFrame()\n",
    "\n",
    "    # One-hot encoding을 위한 라벨 목록 정의\n",
    "    labels = [\n",
    "        \"모욕\", \"욕설\", \"외설\", \"폭력위협/범죄조장\", \"성혐오\",\n",
    "        \"연령\", \"인종/지역\", \"장애\", \"종교\", \"정치성향\", \"직업\"\n",
    "    ]\n",
    "\n",
    "    # One-hot encoding 열 추가\n",
    "    for label in tqdm(labels, \"Labels\"):\n",
    "        saved[f'{label}'] = df[label].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "    \n",
    "    # 원문 및 위치 정보 추가\n",
    "    saved['문장'] = df['문장']\n",
    "\n",
    "    return saved\n",
    "\n",
    "# JSON 파일 로드\n",
    "json_files_path = 'data/selectstar/C*.json'\n",
    "data = load_json_files(json_files_path)\n",
    "\n",
    "# 원본 데이터로부터 One-hot encoding DataFrame 생성\n",
    "one_hot_df = create_one_hot_encoding(data)\n",
    "\n",
    "# 결과 출력 및 CSV 파일로 저장(optional)\n",
    "print(one_hot_df.head(n=10))\n",
    "one_hot_df.to_csv('selectstar-encoded.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      문장  \\\n",
      "0      국민들은 깨어나야 합니다. 지금 민주당을 지지하는 사림들 보세요~ 종북주의자들이 판...   \n",
      "1                          저 어른들...지금 정부 ''꼬라지''' 보는것같구만   \n",
      "2      ''대깨문''' 출동 이번 코로나가 확산되는건 민노총 때문이라고 몰고가라~~~ 눼눼...   \n",
      "3                                아프간과 비교를...ㅎ ''치매드셨'''나   \n",
      "4      작년 국민집회때 #@이름#목사 감옥보내더니 ㅋㅋㅋ8000명 민노총은 집회허락하는 이...   \n",
      "...                                                  ...   \n",
      "99995  \\n판결 요약하면 ''기레기'''질은 맞는데 언론의 자유를 위해 봐준다임. 나쁜''...   \n",
      "99996  나라를 위해 이렇게 몸 아끼지않고 헌신하는 애국자가 있는반면 파란지붕아래 ''쓰레기...   \n",
      "99997  정권바뀌면 없어져야할것,,,,아무쓸모없고 25만원 받아야 아무도움안되니 세금이나 그...   \n",
      "99998  그''놈'''의 #@이름#이가 뭐라고 저토록 지긋지긋하게 무죄가 만들어 주고싶어 그...   \n",
      "99999  ''쌍욕달이''' ''깡패''' #@이름#과 ''망언''' ''전문''' ''문빠'...   \n",
      "\n",
      "                                                   욕설 위치  \n",
      "0                                           [(195, 204)]  \n",
      "1                                             [(14, 22)]  \n",
      "2                       [(0, 8), (129, 137), (229, 237)]  \n",
      "3                                             [(13, 22)]  \n",
      "4                                                     []  \n",
      "...                                                  ...  \n",
      "99995            [(9, 17), (43, 49), (57, 63), (68, 74)]  \n",
      "99996                             [(41, 49), (128, 136)]  \n",
      "99997                                         [(56, 63)]  \n",
      "99998  [(1, 7), (47, 56), (70, 78), (138, 144), (144,...  \n",
      "99999   [(0, 9), (10, 17), (25, 32), (33, 40), (41, 48)]  \n",
      "\n",
      "[100000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 예시 DataFrame 생성\n",
    "df = pd.read_csv(\"selectstar-encoded.csv\", index_col=0)\n",
    "\n",
    "# 욕설 패턴 정의 및 시작과 종료 인덱스 찾기\n",
    "def find_offensive_spans(text):\n",
    "    pattern = r\"''(.*?)'''\"\n",
    "    matches = re.finditer(pattern, text)\n",
    "    spans = []\n",
    "    for match in matches:\n",
    "        start, end = match.span()\n",
    "        spans.append((start, end))\n",
    "    return spans\n",
    "\n",
    "# 각 문장에 대해 시작/종료 인덱스 추가\n",
    "df['욕설 위치'] = df['문장'].apply(find_offensive_spans)\n",
    "# 결과 출력\n",
    "print(df[['문장', '욕설 위치']])\n",
    "df.to_csv('./selectstar-location.csv', encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Custom dataset class\n",
    "class OffensiveLanguageDataset(Dataset):\n",
    "    def __init__(self, dataframe: pd.DataFrame, tokenizer, max_len: int = 256):\n",
    "        self.texts = dataframe['문장'].tolist()\n",
    "        self.start_labels = []\n",
    "        self.end_labels = []\n",
    "        self.targets = dataframe['욕설'].tolist()  # 전체 욕설 여부\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "        for index, row in dataframe.iterrows():\n",
    "            # 욕설 위치를 리스트로 파싱: 실제 tuple인지확인\n",
    "            spans = row['욕설 위치']\n",
    "            if isinstance(spans, list) and len(spans) > 0:  # 리스트가 비어있지 않다면\n",
    "                try:\n",
    "                    self.start_labels.append([start for start, end in spans])  # (start, end) 튜플로 분리\n",
    "                    self.end_labels.append([end for start, end in spans])\n",
    "                except ValueError:  # unpacking mistake\n",
    "                    print(f\"Error unpacking spans in row {index}: {spans}\")\n",
    "                    self.start_labels.append([])\n",
    "                    self.end_labels.append([])\n",
    "            else:\n",
    "                self.start_labels.append([])\n",
    "                self.end_labels.append([])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.texts[index]\n",
    "        start_label = self.start_labels[index]\n",
    "        end_label = self.end_labels[index]\n",
    "        target = self.targets[index]\n",
    "\n",
    "        # Tokenize the text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Prepare the return dictionary\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'start_labels': torch.tensor(start_label, dtype=torch.float),  # 장면별 욕설 시작 인덱스\n",
    "            'end_labels': torch.tensor(end_label, dtype=torch.float),      # 장면별 욕설 끝 인덱스\n",
    "            'targets': torch.tensor(target, dtype=torch.float)              # 전체 욕설 여부\n",
    "        }\n",
    "\n",
    "class MultiOffensiveLanguageModel(nn.Module):\n",
    "    def __init__(self, model_name, num_labels=1):\n",
    "        super(MultiOffensiveLanguageModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.classification_layer = nn.Linear(self.bert.config.hidden_size, num_labels)  # 욕설 여부\n",
    "        self.start_layer = nn.Linear(self.bert.config.hidden_size, 1)  # 시작 위치\n",
    "        self.end_layer = nn.Linear(self.bert.config.hidden_size, 1)  # 끝 위치\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.last_hidden_state  # [batch_size, seq_len, hidden_size]\n",
    "\n",
    "        classification_logits = self.classification_layer(hidden_states)  # [batch_size, seq_len, num_labels]\n",
    "        start_logits = self.start_layer(hidden_states)  # [batch_size, seq_len, 1]\n",
    "        end_logits = self.end_layer(hidden_states)  # [batch_size, seq_len, 1]\n",
    "        \n",
    "        return classification_logits, start_logits, end_logits\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "max_len = 256\n",
    "batch_size = 16\n",
    "num_epochs = 5\n",
    "learning_rate = 2e-5\n",
    "\n",
    "# Tokenizer 및 모델 초기화\n",
    "model_name = 'monologg/kobert'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = MultiOffensiveLanguageModel(model_name)\n",
    "\n",
    "# 데이터셋 및 데이터로더 준비\n",
    "df = pd.read_csv('selectstar-location.csv')\n",
    "\n",
    "dataset = OffensiveLanguageDataset(df, tokenizer, max_len)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Optimizer 및 Loss Function 설정\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "loss_function_cls = nn.BCEWithLogitsLoss()  # 욕설 여부에 대한 손실 함수\n",
    "loss_function_loc = nn.MSELoss()  # 시작 및 끝 위치에 대한 손실 함수\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        start_labels = batch['start_labels']\n",
    "        end_labels = batch['end_labels']\n",
    "        targets = batch['targets']\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        classification_logits, start_logits, end_logits = model(input_ids, attention_mask)\n",
    "\n",
    "        # 손실 계산\n",
    "        # Reshape outputs to match the targets shape\n",
    "        loss_cls = loss_function_cls(classification_logits.view(-1, 1), targets.unsqueeze(1).view(-1, 1))\n",
    "        loss_start = loss_function_loc(start_logits, start_labels.view(-1, 1))\n",
    "        loss_end = loss_function_loc(end_logits, end_labels.view(-1, 1))\n",
    "\n",
    "        loss = loss_cls + loss_start + loss_end\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.4f}')\n",
    "\n",
    "# 모델 학습이 완료되었습니다.\n",
    "\n",
    "# 모델 저장\n",
    "model_save_path = './my_model'\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "print(f'Model saved at {model_save_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "filter-abuse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
